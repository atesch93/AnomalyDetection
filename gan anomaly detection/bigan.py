# -*- coding: utf-8 -*-
"""bigan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FWXD5_oGC-USi8oBMqXOFlYKDmOYjjlU
"""

mnist=input_data1.read_data_sets('/content/data',one_hot=True)
#!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz
#!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz
#!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz
#!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz

import tensorflow.compat.v1 as tf
import tensorflow_datasets
input_data = tensorflow_datasets.load('mnist')
result = input_data.items()
  
# Convert object to a list
data = list(result)
  
# Convert list to an array
numpyArray = np.array(data)
  
input_data1=numpyArray


tf.disable_v2_behavior()
import matplotlib.pyplot as plt
import numpy as np
print(input_data1)

!mkdir MNIST_Fashion
! cp *.gz MNIST_Fashion/
import tensorflow_datasets
mnist = tensorflow_datasets.load('MNIST_Fashion1/')

#training parameters
learning_rate=0.002
batch_size=32
epochs=1000
Z_dim=64
#network parameters
image_dimension=64      #image size is 28x28
#discriminator nodes
D_H1=256
#generator nodes
G_H1=256
#encoder network
E_H1=256

#z1 initialisation
def xavier_init(shape):
  return tf.random.normal(shape=shape,stddev=1./tf.sqrt(shape[0]/2.0))

#define weights and bias dictionaries for disriminator

Disc_W={"disc_H"     :tf.Variable(xavier_init([image_dimension +Z_dim,D_H1])),
       "disc_final" :tf.Variable(xavier_init([D_H1,1])),
       }

Disc_Bias={"disc_H"     :tf.Variable(xavier_init([D_H1])),
       "disc_final" :tf.Variable(xavier_init([1])),
       }

#define weights and bias dictionaries for generator

Gen_W={"Gen_H"     :tf.Variable(xavier_init([Z_dim,G_H1])),#16+10 is due to z and c dimension combined
       "Gen_final" :tf.Variable(xavier_init([G_H1,image_dimension])),#784 due to output dimension of image
       }

Gen_Bias={"Gen_H"     :tf.Variable(xavier_init([G_H1])),
       "Gen_final" :tf.Variable(xavier_init([image_dimension])),
       }

#define weights and bias dictionaries for encoder

E_W={"E_H"     :tf.Variable(xavier_init([image_dimension,E_H1])),#784 is due to z and c dimension combined
       "E_final" :tf.Variable(xavier_init([E_H1,Z_dim])),#10 due to output dimension of image
       }

E_Bias={"E_H"     :tf.Variable(xavier_init([E_H1])),#784 is due to z and c dimension combined
       "E_final" :tf.Variable(xavier_init([Z_dim])),#10 due to output dimension of image
       }

#defining placeholder


tf.compat.v1.disable_eager_execution()

X=tf.placeholder(tf.float32, shape=(None,image_dimension), name='tensor1')
Z=tf.placeholder(tf.float32, shape=(None,Z_dim), name='tensor1')

def Z_distribution(m,n):
  return np.random.uniform(-1.,1.,size=[m,n])

#Creating the computational graph
def Generator(z):
  G_hidden_layer=tf.nn.relu(tf.add(tf.matmul(z,Gen_W["Gen_H"]),Gen_Bias["Gen_H"]))
  gen_output=tf.add(tf.matmul(G_hidden_layer,Gen_W["Gen_final"]),Gen_Bias["Gen_final"])
  gen_prob_output=tf.nn.sigmoid(gen_output)
  return gen_prob_output

def Discriminator(x,z):
  input=tf.concat(axis=1,values=[x,z])
  D_hidden_layer=tf.nn.relu(tf.add(tf.matmul(input,Disc_W["disc_H"]),Disc_Bias["disc_H"]))
  disc_output=tf.add(tf.matmul(D_hidden_layer,Disc_W["disc_final"]),Disc_Bias["disc_final"])
  disc_prob_output=tf.nn.sigmoid(disc_output)
  return disc_prob_output

def Encoder_NN(x):
  Q_hidden_layer=tf.nn.relu(tf.add(tf.matmul(x,E_W["E_H"]),E_Bias["E_H"]))
  Q_output=tf.add(tf.matmul(Q_hidden_layer,E_W["E_final"]),E_Bias["E_final"])
  Q_prob_output=(Q_output)
  return Q_prob_output

#Buildin Bigan
#genrator part

z_generated=Encoder_NN(X)
x_generated=Generator(Z)
#Discriminator Network
real_output_Disc=Discriminator(X,z_generated)
fake_output_Disc=Discriminator(x_generated,Z)
tf.disable_v2_behavior()

Disc_Loss=-tf.reduce_mean(tf.log(real_output_Disc+1e-7)+ tf.log(1.0-fake_output_Disc+1e-7))
Gen_Loss=-tf.reduce_mean(tf.log(fake_output_Disc+1e-7)+ tf.log(1.0-real_output_Disc+1e-7))

#Discriminator parameter list
Disc_param=[Disc_W["disc_H"],Disc_W["disc_final"],Disc_Bias["disc_H"],Disc_Bias["disc_final"]]
#Gen parameter list
Gen_param=[Gen_W["Gen_H"],Gen_W["Gen_final"],Gen_Bias["Gen_H"],Gen_Bias["Gen_final"]]
#Encoder parameter list
E_param=E_param=[E_W["E_H"],E_W["E_final"],E_Bias["E_H"],E_Bias["E_final"]]
#optimiser
Gen_optimize=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(Gen_Loss,var_list=Gen_param+E_param)
Disc_optimize=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(Disc_Loss,var_list=Disc_param)

#覺n覺t覺al覺ser
init=tf.global_variables_initializer()
sess=tf.compat.v1.Session()
sess.run(init)

for epoch in range(epochs):
  X_batch,_=mnist_train.train.next_batch(batch_size)
  Z_noise=Z_distribution(batch_size,Z_dim)

  _,Disc_loss_epoch=sess.run([Disc_optimize,Disc_Loss],feed_dict={X:X_batch,Z:Z_noise})
  _,Gen_loss_epoch=sess.run([Gen_optimize,Gen_Loss],feed_dict={X:X_batch,Z:Z_noise})

  if epoch%2000==0:
    print("Steps:{0},Disc_Loss:{1},Gen_loss:{2}".format(epoch,Disc_loss_epoch,Gen_loss_epoch))

#testing generate images from noise , using generator

test_output=Generator(Z)
n=6
canvas=np.empty((28*n,28*n))
for i in range(n):
  #noise input
  Z_noise=Z_distribution(batch_size,Z_dim)
  #generate image from noise
  g=sess.run(test_output,feed_dict={Z:Z_noise})
  #reverse colour for better display
  for j in range(n):
    #draw generated digits
    canvas[i*28:(i+1)*28,j*28:(j+1)*28]=g[j].reshape([28,28])

  plt.figure(figsize(n,n))
  plt.imshow(canvas,origin="upper",cmap="gray")
  plt.show

#generate images from noise ,using the generator network

z_generated=Encoder_NN(X)
test_output=Generator(z_generated)
n=6
canvas1=np.empty((28*n,28*n))
canvas2=np.empty((28*n,28*n))

